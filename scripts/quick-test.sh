#!/bin/bash

# Jetson Ollama å¿«é€ŸåŠŸèƒ½æµ‹è¯•è„šæœ¬
# ç”¨äºéªŒè¯jetson-containerså’Œollamaçš„å„é¡¹åŠŸèƒ½

echo "ğŸš€ Jetson Ollama å¿«é€ŸåŠŸèƒ½æµ‹è¯•"
echo "=============================="
echo "æ—¶é—´: $(date)"
echo

# 1. æ£€æŸ¥autotagåŠŸèƒ½
echo "1ï¸âƒ£ æµ‹è¯•autotagåŠŸèƒ½"
echo "â–¶ï¸ æ‰§è¡Œ: autotag ollama"
autotag ollama
echo

# 2. æ£€æŸ¥å®¹å™¨çŠ¶æ€
echo "2ï¸âƒ£ æ£€æŸ¥å®¹å™¨çŠ¶æ€"
echo "â–¶ï¸ æ‰§è¡Œ: docker ps | grep ollama"
docker ps | grep ollama
if [ $? -eq 0 ]; then
    echo "âœ… Ollamaå®¹å™¨æ­£åœ¨è¿è¡Œ"
else
    echo "âŒ Ollamaå®¹å™¨æœªè¿è¡Œ"
fi
echo

# 3. æ£€æŸ¥APIå¯ç”¨æ€§
echo "3ï¸âƒ£ æµ‹è¯•APIå¯ç”¨æ€§"
echo "â–¶ï¸ æ‰§è¡Œ: curl -s http://localhost:11434/api/tags"
API_RESPONSE=$(curl -s http://localhost:11434/api/tags)
if [ $? -eq 0 ] && [[ $API_RESPONSE == *"models"* ]]; then
    echo "âœ… Ollama APIæ­£å¸¸å“åº”"
    echo "ğŸ“‹ å¯ç”¨æ¨¡å‹æ•°é‡: $(echo $API_RESPONSE | grep -o '"name"' | wc -l)"
else
    echo "âŒ Ollama APIæ— å“åº”"
fi
echo

# 4. æ£€æŸ¥GPUçŠ¶æ€
echo "4ï¸âƒ£ æ£€æŸ¥GPUçŠ¶æ€"
echo "â–¶ï¸ æ‰§è¡Œ: docker exec ollama-container nvidia-smi"
docker exec ollama-container nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
if [ $? -eq 0 ]; then
    echo "âœ… GPUåœ¨å®¹å™¨å†…æ­£å¸¸å·¥ä½œ"
else
    echo "âŒ GPUåœ¨å®¹å™¨å†…ä¸å¯ç”¨"
fi
echo

# 5. å¿«é€Ÿæ¨ç†æµ‹è¯•
echo "5ï¸âƒ£ å¿«é€Ÿæ¨ç†æµ‹è¯•"
echo "â–¶ï¸ æµ‹è¯•ç®€å•æ¨ç†..."
INFERENCE_RESULT=$(curl -s -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "smollm2:135m", "prompt": "Hello", "stream": false}' | head -c 100)

if [[ $INFERENCE_RESULT == *"response"* ]]; then
    echo "âœ… æ¨ç†åŠŸèƒ½æ­£å¸¸"
else
    echo "âŒ æ¨ç†åŠŸèƒ½å¼‚å¸¸"
fi
echo

# 6. ç³»ç»Ÿèµ„æºä½¿ç”¨
echo "6ï¸âƒ£ ç³»ç»Ÿèµ„æºä½¿ç”¨"
echo "â–¶ï¸ å†…å­˜ä½¿ç”¨æƒ…å†µ:"
free -h | head -2
echo "â–¶ï¸ ç£ç›˜ä½¿ç”¨æƒ…å†µ:"
df -h | grep -E "(æ–‡ä»¶ç³»ç»Ÿ|Filesystem|/dev/)"
echo

echo "ğŸ¯ æµ‹è¯•å®Œæˆï¼"
echo "å¦‚æœæ‰€æœ‰é¡¹ç›®éƒ½æ˜¾ç¤º âœ…ï¼Œè¯´æ˜ç³»ç»Ÿè¿è¡Œæ­£å¸¸ã€‚"
echo "å¦‚æœæœ‰ âŒ é¡¹ç›®ï¼Œè¯·å‚è€ƒéƒ¨ç½²æ—¥å¿—è¿›è¡Œæ•…éšœæ’é™¤ã€‚" 